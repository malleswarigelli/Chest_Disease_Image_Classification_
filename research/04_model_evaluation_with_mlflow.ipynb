{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\anjik\\\\Desktop\\\\MLOPs_projects\\\\Chest_Disease_Image_Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\anjik\\\\Desktop\\\\MLOPs_projects\\\\Chest_Disease_Image_Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "from cnnClassifier import logger\n",
    "import tensorflow as tf\n",
    "\n",
    "from cnnClassifier.utils.common import save_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set mlflow tracking uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/malleswarigelli/Chest_Disease_Image_Classification_.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"malleswarigelli\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"2285aa424e395caab6840e555b7d9680a386cde7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "import tensorflow as tf\n",
    "#model = tf.keras.models.load_model(\"artifacts/model_training/model.h5\")\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config_entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    path_of_trained_model: Path\n",
    "    training_data: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_batch_size: int    \n",
    "    params_image_size: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write configuration manager\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager class captures & returns configuration for components implementation\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        # params: config.yaml, params.yaml file paths          \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        # read config, params yaml file\n",
    "        self.config = read_yaml(config_filepath) # returns ConfigBox to access data easily\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # create artifact directory\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_evaluation_config (self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Method: get_model_evaluation_config\n",
    "        Params:\n",
    "        Returns: configuration for Model Evaluation component i.e ModelEvaluationConfig \n",
    "        \"\"\"\n",
    "        logger.info(\"Entering get_model_evaluation_config method of ConfigurationManager\")\n",
    "        model_evaluation_config = ModelEvaluationConfig(path_of_trained_model= \"artifacts/model_training/model.h5\", \n",
    "                                                        training_data= \"artifacts/data_ingestion/Chest-CT-Scan-data\",                                                         \n",
    "                                                        mlflow_uri= \"https://dagshub.com/malleswarigelli/Chest_Disease_Image_Classification_.mlflow\", \n",
    "                                                        all_params= self.params, \n",
    "                                                        params_batch_size= self.params.BATCH_SIZE, \n",
    "                                                        params_image_size= self.params.IMAGE_SIZE\n",
    "            \n",
    "        )\n",
    "        logger.info(\"Exiting get_model_evaluation_config method of ConfigurationManager\")\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    \"\"\"\n",
    "     This class tracks model evaluation metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, evaluation_config:ModelEvaluationConfig):\n",
    "        self.evaluation_config= evaluation_config \n",
    "        \n",
    "    \n",
    "    def _valid_generator(self):\n",
    "        \"\"\"\n",
    "        Method Name : _valid_generator\n",
    "        Description : performs split of training data, slice 30% data for validation \n",
    "        Output      : \n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \n",
    "        \"\"\"\n",
    "        logger.info(\"Data split for validation data started!\")\n",
    "        data_generator_kwargs= dict(rescale= 1./255,\n",
    "                                   validation_split= 0.30)\n",
    "        \n",
    "        dataflow_kwargs= dict(target_size= self.evaluation_config.params_image_size[:-1],\n",
    "                              batch_size= self.evaluation_config.params_batch_size,\n",
    "                              interpolation= \"bilinear\")\n",
    "        \n",
    "        valid_data_generator= tf.keras.preprocessing.image.ImageDataGenerator(**data_generator_kwargs)\n",
    "        self.valid_generator= valid_data_generator.flow_from_directory(directory= self.evaluation_config.training_data,\n",
    "                                                                       subset= \"validation\",\n",
    "                                                                       shuffle= False,\n",
    "                                                                       **dataflow_kwargs)\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "        \n",
    "    def evaluation(self):\n",
    "        \"\"\"\n",
    "        Method Name : evaluation\n",
    "        Description : performs model evaluation\n",
    "        Output      : \n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \n",
    "        \"\"\"\n",
    "        logger.info(\"Model evaluation started!\")\n",
    "        logger.info(\"Loading the Trained model\")\n",
    "        self.model= self.load_model(self.evaluation_config.path_of_trained_model)\n",
    "        logger.info(\"Generating validation data\")\n",
    "        self._valid_generator()\n",
    "        logger.info(\"Evaluating the model, saving loss and accuracy scores\")\n",
    "        self.metric= model.evaluate(self.valid_generator)\n",
    "        self.save_metrics()\n",
    "        \n",
    "    def save_metrics(self):\n",
    "        \"\"\"\n",
    "        Method Name : save_score\n",
    "        Description : saves loss and accuracy scores\n",
    "        Output      : \n",
    "        On Failure  : Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        metrics= {\"loss\": self.metric[0], \"accuracy\": self.metric[1]}\n",
    "        save_json(path= Path(\"scores.json\"), data= metrics)        \n",
    "        \n",
    "        \n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"\n",
    "        Method Name : log_into_mlflow\n",
    "        Description : log experiments in mlflow\n",
    "        Output      : \n",
    "        On Failure  : Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        logger.info(\"Logging Experiments stated in mlflow\")\n",
    "        mlflow.set_registry_uri(self.evaluation_config.mlflow_uri)\n",
    "        tracking_url_type_store= urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            logger.info(\"Logging parameters metrics\")\n",
    "            mlflow.log_params(self.evaluation_config.all_params)\n",
    "            mlflow.log_metrics(\n",
    "                {\"loss\":self.metric[0],\n",
    "                 \"accuracy\":self.metric[1]}\n",
    "            )\n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.sklearn.log_model(self.model, \"model\", registered_model_name=\"Vgg16Model\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(self.model, \"model\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-17 19:05:58,038: INFO: 3925082432: ModelEvaluation component started]\n",
      "[2024-03-17 19:05:58,039: INFO: 3925082432: Loading of ModelEvaluation component configuration started]\n",
      "[2024-03-17 19:05:58,042: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-03-17 19:05:58,044: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-17 19:05:58,046: INFO: common: created directory at: artifacts]\n",
      "[2024-03-17 19:05:58,046: INFO: 3105106954: Entering get_model_evaluation_config method of ConfigurationManager]\n",
      "[2024-03-17 19:05:58,047: INFO: 3105106954: Exiting get_model_evaluation_config method of ConfigurationManager]\n",
      "[2024-03-17 19:05:58,048: INFO: 3925082432: All configuration directories, files needed for ModelEvaluation component are ready]\n",
      "[2024-03-17 19:05:58,048: INFO: 3925082432: ModelEvaluation steps started]\n",
      "[2024-03-17 19:05:58,049: INFO: 1091401809: Model evaluation started!]\n",
      "[2024-03-17 19:05:58,050: INFO: 1091401809: Loading the Trained model]\n",
      "[2024-03-17 19:05:58,474: INFO: 1091401809: Generating validation data]\n",
      "[2024-03-17 19:05:58,475: INFO: 1091401809: Data split for validation data started!]\n",
      "Found 102 images belonging to 2 classes.\n",
      "[2024-03-17 19:05:58,489: INFO: 1091401809: Evaluating the model, saving loss and accuracy scores]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK! ModelEvaluation component completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelEvaluation steps started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m model_evaluation \u001b[38;5;241m=\u001b[39m ModelEvaluation(evaluation_config\u001b[38;5;241m=\u001b[39mevaluation_config) \u001b[38;5;66;03m# create object for ModelTraining class\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel_evaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# obj.method()    \u001b[39;00m\n\u001b[0;32m     11\u001b[0m model_evaluation\u001b[38;5;241m.\u001b[39mlog_into_mlflow()\n\u001b[0;32m     13\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK! ModelEvaluation component completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m, in \u001b[0;36mModelEvaluation.evaluation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_generator()\n\u001b[0;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model, saving loss and accuracy scores\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_generator)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_metrics()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(\"ModelEvaluation component started\")\n",
    "    logger.info(\"Loading of ModelEvaluation component configuration started\")\n",
    "    config = ConfigurationManager() # create object for ConfigurationManager class\n",
    "    evaluation_config= config.get_model_evaluation_config() # obj.method() returns ModelEvaluationConfig\n",
    "    logger.info(\"All configuration directories, files needed for ModelEvaluation component are ready\")\n",
    "    \n",
    "    logger.info(\"ModelEvaluation steps started\")\n",
    "    model_evaluation = ModelEvaluation(evaluation_config=evaluation_config) # create object for ModelTraining class\n",
    "    model_evaluation.evaluation() # obj.method()    \n",
    "    model_evaluation.log_into_mlflow()\n",
    "\n",
    "    logger.info(\"OK! ModelEvaluation component completed\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
