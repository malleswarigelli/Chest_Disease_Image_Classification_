{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\anjik\\\\Desktop\\\\MLOPs_projects\\\\Chest_Disease_Image_Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\anjik\\\\Desktop\\\\MLOPs_projects\\\\Chest_Disease_Image_Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "from cnnClassifier import logger\n",
    "import tensorflow as tf\n",
    "\n",
    "from cnnClassifier.utils.common import save_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set mlflow tracking uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/malleswarigelli/Chest_Disease_Image_Classification_.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"malleswarigelli\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"2285aa424e395caab6840e555b7d9680a386cde7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "import tensorflow as tf\n",
    "#model = tf.keras.models.load_model(\"artifacts/model_training/model.h5\")\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config_entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    path_of_trained_model: Path\n",
    "    training_data: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_batch_size: int    \n",
    "    params_image_size: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write configuration manager\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager class captures & returns configuration for components implementation\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        # params: config.yaml, params.yaml file paths          \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        # read config, params yaml file\n",
    "        self.config = read_yaml(config_filepath) # returns ConfigBox to access data easily\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # create artifact directory\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_evaluation_config (self) -> ModelEvaluationConfig:\n",
    "        \"\"\"\n",
    "        Method: get_model_evaluation_config\n",
    "        Params:\n",
    "        Returns: configuration for Model Evaluation component i.e ModelEvaluationConfig \n",
    "        \"\"\"\n",
    "        logger.info(\"Entering get_model_evaluation_config method of ConfigurationManager\")\n",
    "        model_evaluation_config = ModelEvaluationConfig(path_of_trained_model= \"artifacts/model_training/model.h5\", \n",
    "                                                        training_data= \"artifacts/data_ingestion/Chest-CT-Scan-data\",                                                         \n",
    "                                                        mlflow_uri= \"https://dagshub.com/malleswarigelli/Chest_Disease_Image_Classification_.mlflow\", \n",
    "                                                        all_params= self.params, \n",
    "                                                        params_batch_size= self.params.BATCH_SIZE, \n",
    "                                                        params_image_size= self.params.IMAGE_SIZE\n",
    "            \n",
    "        )\n",
    "        logger.info(\"Exiting get_model_evaluation_config method of ConfigurationManager\")\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    \"\"\"\n",
    "     This class tracks model evaluation metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, evaluation_config:ModelEvaluationConfig):\n",
    "        self.evaluation_config= evaluation_config \n",
    "        \n",
    "    \n",
    "    def _valid_generator(self):\n",
    "        \"\"\"\n",
    "        Method Name : _valid_generator\n",
    "        Description : performs split of training data, slice 30% data for validation \n",
    "        Output      : \n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \n",
    "        \"\"\"\n",
    "        logger.info(\"Data split for validation data started!\")\n",
    "        data_generator_kwargs= dict(rescale= 1./255,\n",
    "                                   validation_split= 0.30)\n",
    "        \n",
    "        dataflow_kwargs= dict(target_size= self.evaluation_config.params_image_size[:-1],\n",
    "                              batch_size= self.evaluation_config.params_batch_size,\n",
    "                              interpolation= \"bilinear\")\n",
    "        \n",
    "        valid_data_generator= tf.keras.preprocessing.image.ImageDataGenerator(**data_generator_kwargs)\n",
    "        self.valid_generator= valid_data_generator.flow_from_directory(directory= self.evaluation_config.training_data,\n",
    "                                                                       subset= \"validation\",\n",
    "                                                                       shuffle= False,\n",
    "                                                                       **dataflow_kwargs)\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "        \n",
    "    def evaluation(self):\n",
    "        \"\"\"\n",
    "        Method Name : evaluation\n",
    "        Description : performs model evaluation\n",
    "        Output      : \n",
    "        On Failure  :   Write an exception log and then raise an exception\n",
    "        \n",
    "        \"\"\"\n",
    "        logger.info(\"Model evaluation started!\")\n",
    "        logger.info(\"Loading the Trained model\")\n",
    "        self.model= self.load_model(self.evaluation_config.path_of_trained_model)\n",
    "        logger.info(\"Generating validation data\")\n",
    "        self._valid_generator()\n",
    "        logger.info(\"Evaluating the model, saving loss and accuracy scores\")\n",
    "        self.metric= self.model.evaluate(self.valid_generator)\n",
    "        self.save_metrics()\n",
    "        \n",
    "    def save_metrics(self):\n",
    "        \"\"\"\n",
    "        Method Name : save_score\n",
    "        Description : saves loss and accuracy scores\n",
    "        Output      : \n",
    "        On Failure  : Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        metrics= {\"loss\": self.metric[0], \"accuracy\": self.metric[1]}\n",
    "        save_json(path= Path(\"scores.json\"), data= metrics)        \n",
    "        \n",
    "        \n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"\n",
    "        Method Name : log_into_mlflow\n",
    "        Description : log experiments in mlflow\n",
    "        Output      : \n",
    "        On Failure  : Write an exception log and then raise an exception\n",
    "        \"\"\"\n",
    "        logger.info(\"Logging Experiments stated in mlflow\")\n",
    "        mlflow.set_registry_uri(self.evaluation_config.mlflow_uri)\n",
    "        tracking_url_type_store= urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            logger.info(\"Logging parameters metrics\")\n",
    "            mlflow.log_params(self.evaluation_config.all_params)\n",
    "            mlflow.log_metrics(\n",
    "                {\"loss\":self.metric[0],\n",
    "                 \"accuracy\":self.metric[1]}\n",
    "            )\n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.sklearn.log_model(self.model, \"model\", registered_model_name=\"Vgg16Model\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(self.model, \"model\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-18 11:01:18,461: INFO: 1570068939: ModelEvaluation component started]\n",
      "[2024-03-18 11:01:18,462: INFO: 1570068939: Loading of ModelEvaluation component configuration started]\n",
      "[2024-03-18 11:01:18,466: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-03-18 11:01:18,469: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-18 11:01:18,469: INFO: common: created directory at: artifacts]\n",
      "[2024-03-18 11:01:18,470: INFO: 3105106954: Entering get_model_evaluation_config method of ConfigurationManager]\n",
      "[2024-03-18 11:01:18,471: INFO: 3105106954: Exiting get_model_evaluation_config method of ConfigurationManager]\n",
      "[2024-03-18 11:01:18,472: INFO: 1570068939: All configuration directories, files needed for ModelEvaluation component are ready]\n",
      "[2024-03-18 11:01:18,473: INFO: 1570068939: ModelEvaluation steps started]\n",
      "[2024-03-18 11:01:18,474: INFO: 916769282: Model evaluation started!]\n",
      "[2024-03-18 11:01:18,479: INFO: 916769282: Loading the Trained model]\n",
      "[2024-03-18 11:01:19,073: INFO: 916769282: Generating validation data]\n",
      "[2024-03-18 11:01:19,076: INFO: 916769282: Data split for validation data started!]\n",
      "Found 102 images belonging to 2 classes.\n",
      "[2024-03-18 11:01:19,099: INFO: 916769282: Evaluating the model, saving loss and accuracy scores]\n",
      "7/7 [==============================] - 18s 3s/step - loss: 15.7934 - accuracy: 0.4314\n",
      "[2024-03-18 11:01:37,438: INFO: common: json file saved at: scores.json]\n",
      "[2024-03-18 11:01:37,443: INFO: common: json file saved at: scores.json]\n",
      "[2024-03-18 11:01:37,445: INFO: 916769282: Logging Experiments stated in mlflow]\n",
      "[2024-03-18 11:01:38,796: INFO: 916769282: Logging parameters metrics]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/18 11:01:47 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\anjik\\AppData\\Local\\Temp\\tmpvgjlpsju\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
      "c:\\Users\\anjik\\anaconda3\\envs\\chest\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'Vgg16Model' already exists. Creating a new version of this model...\n",
      "2024/03/18 11:12:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: Vgg16Model, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-18 11:12:43,216: INFO: 1570068939: OK! ModelEvaluation component completed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'Vgg16Model'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(\"ModelEvaluation component started\")\n",
    "    logger.info(\"Loading of ModelEvaluation component configuration started\")\n",
    "    config = ConfigurationManager() # create object for ConfigurationManager class\n",
    "    evaluation_config= config.get_model_evaluation_config() # obj.method() returns ModelEvaluationConfig\n",
    "    logger.info(\"All configuration directories, files needed for ModelEvaluation component are ready\")\n",
    "    logger.info(\"ModelEvaluation steps started\")\n",
    "    model_evaluation = ModelEvaluation(evaluation_config=evaluation_config) # create object for ModelTraining class\n",
    "    model_evaluation.evaluation() # obj.method() \n",
    "    model_evaluation.save_metrics()   \n",
    "    model_evaluation.log_into_mlflow()\n",
    "    \n",
    "    logger.info(\"OK! ModelEvaluation component completed\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
